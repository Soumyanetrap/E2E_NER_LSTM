{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fdac849beb0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from dataclasses import dataclass\n","import torch\n","import torchaudio\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import re\n","from torch.utils.data import DataLoader\n","\n","torch.random.manual_seed(0)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["dataset_path_base = '../dataset/fluent_speech_commands_dataset/'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def hook(module, input, output):\n","    global layer_output\n","    layer_output = output"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_trellis(emission, tokens, blank_id=0):\n","    num_frame = emission.size(0)\n","    num_tokens = len(tokens)\n","\n","    trellis = torch.zeros((num_frame, num_tokens))\n","    trellis[1:, 0] = torch.cumsum(emission[1:, blank_id], 0)\n","    trellis[0, 1:] = -float(\"inf\")\n","    trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n","\n","    for t in range(num_frame - 1):\n","        trellis[t + 1, 1:] = torch.maximum(\n","            # Score for staying at the same token\n","            trellis[t, 1:] + emission[t, blank_id],\n","            # Score for changing to the next token\n","            trellis[t, :-1] + emission[t, tokens[1:]],\n","        )\n","    return trellis"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["@dataclass\n","class Point:\n","    token_index: int\n","    time_index: int\n","    score: float\n","\n","\n","def backtrack(trellis, emission, tokens, blank_id=0):\n","    t, j = trellis.size(0) - 1, trellis.size(1) - 1\n","\n","    path = [Point(j, t, emission[t, blank_id].exp().item())]\n","    while j > 0:\n","        # Should not happen but just in case\n","        assert t > 0\n","\n","        # 1. Figure out if the current position was stay or change\n","        # Frame-wise score of stay vs change\n","        p_stay = emission[t - 1, blank_id]\n","        p_change = emission[t - 1, tokens[j]]\n","\n","        # Context-aware score for stay vs change\n","        stayed = trellis[t - 1, j] + p_stay\n","        changed = trellis[t - 1, j - 1] + p_change\n","\n","        # Update position\n","        t -= 1\n","        if changed > stayed:\n","            j -= 1\n","\n","        # Store the path with frame-wise probability.\n","        prob = (p_change if changed > stayed else p_stay).exp().item()\n","        path.append(Point(j, t, prob))\n","\n","    # Now j == 0, which means, it reached the SoS.\n","    # Fill up the rest for the sake of visualization\n","    while t > 0:\n","        prob = emission[t - 1, blank_id].exp().item()\n","        path.append(Point(j, t - 1, prob))\n","        t -= 1\n","\n","    return path[::-1]\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Merge the labels\n","@dataclass\n","class Segment:\n","    label: str\n","    start: int\n","    end: int\n","    score: float\n","\n","    def __repr__(self):\n","        return f\"{self.label} ({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n","\n","    @property\n","    def length(self):\n","        return self.end - self.start\n","\n","\n","def merge_repeats(path, transcript):\n","    i1, i2 = 0, 0\n","    segments = []\n","    while i1 < len(path):\n","        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n","            i2 += 1\n","        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n","        segments.append(\n","            Segment(\n","                transcript[path[i1].token_index],\n","                path[i1].time_index,\n","                path[i2 - 1].time_index + 1,\n","                score,\n","            )\n","        )\n","        i1 = i2\n","    return segments"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Merge words\n","def merge_words(segments, separator=\"|\"):\n","    words = []\n","    i1, i2 = 0, 0\n","    while i1 < len(segments):\n","        if i2 >= len(segments) or segments[i2].label == separator:\n","            if i1 != i2:\n","                segs = segments[i1:i2]\n","                word = \"\".join([seg.label for seg in segs])\n","                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n","                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n","            i1 = i2 + 1\n","            i2 = i1\n","        else:\n","            i2 += 1\n","    return words\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def getAudioAndTranscripts(file, label2id):\n","    SPEECH_FILES = []\n","    TRANSCRIPTS = []\n","    TARGETS = []\n","    df = pd.read_csv(os.path.join(dataset_path_base+'/data/',file))\n","\n","    for i in tqdm(range(0,len(df))):\n","        audio_file = os.path.join(dataset_path_base,df.loc[i,'path'])\n","        SPEECH_FILES.append(audio_file)\n","        TRANSCRIPTS.append(df.loc[i,'transcription'])\n","        object = df.loc[i,'object']\n","        location = df.loc[i,'location']\n","        labels = []\n","        for v in TRANSCRIPTS[i].split(\" \"):\n","            if(v == object):\n","                labels.append(label2id['object'])\n","            elif(v == location):\n","                labels.append(label2id['location'])\n","            else:\n","                labels.append(label2id['O'])\n","        TARGETS.append([*labels])\n","    return SPEECH_FILES, TRANSCRIPTS, TARGETS"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 4227/23132 [00:00<00:00, 42264.90it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 23132/23132 [00:00<00:00, 42740.61it/s]\n","100%|██████████| 3118/3118 [00:00<00:00, 42131.40it/s]\n","100%|██████████| 3793/3793 [00:00<00:00, 43572.56it/s]\n"]}],"source":["label_names = ['O','object', 'location']\n","    \n","label2id = {k: v for v, k in enumerate(label_names)}\n","id2label = {v: k for v, k in enumerate(label_names)}\n","\n","audio_paths_train, transcripts_train, targets_train = getAudioAndTranscripts('train_data.csv', label2id)\n","audio_paths_valid, transcripts_valid, targets_valid = getAudioAndTranscripts('valid_data.csv', label2id)\n","audio_paths_test, transcripts_test, targets_test = getAudioAndTranscripts('test_data.csv', label2id)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# class dataset(Dataset):\n","#     def __init__(self, audio_paths, transcripts, targets, max_len):\n","#         self.len = len(audio_paths)\n","#         self.audio_paths = audio_paths\n","#         self.transcripts = transcripts\n","#         self.targets = targets\n","#         self.max_len = max_len\n","        \n","#     def __getitem__(self, index):\n","#         # step 1: tokenize (and adapt corresponding labels)\n","#         sentence = self.data.transcription[index]  \n","#         word_labels = self.data.labels[index]  \n","#         tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n","        \n","#         # step 2: add special tokens (and corresponding labels)\n","#         tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n","#         print(tokenized_sentence)\n","#         labels.insert(0, \"O\") # add outside label for [CLS] token\n","#         labels.insert(-1, \"O\") # add outside label for [SEP] token\n","\n","#         # step 3: truncating/padding\n","#         maxlen = self.max_len\n","\n","#         if (len(tokenized_sentence) > maxlen):\n","#           # truncate\n","#           tokenized_sentence = tokenized_sentence[:maxlen]\n","#           labels = labels[:maxlen]\n","#         else:\n","#           # pad\n","#           tokenized_sentence = tokenized_sentence + ['[PAD]' for _ in range(maxlen - len(tokenized_sentence))]\n","#           labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n","\n","#         # step 4: obtain the attention mask\n","#         attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","        \n","#         # step 5: convert tokens to input ids\n","#         ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","#         print(ids)\n","\n","#         label_ids = [label2id[label] for label in labels]\n","        \n","#         return {\n","#               'ids': torch.tensor(ids, dtype=torch.long),\n","#               'mask': torch.tensor(attn_mask, dtype=torch.long),\n","#               #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","#               'targets': torch.tensor(label_ids, dtype=torch.long)\n","#         } \n","    \n","    \n","#     def __len__(self):\n","#         return self.len\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 92, 29])\n"]}],"source":["bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n","model = bundle.get_model().to(device)\n","labels = bundle.get_labels()\n","maxlen = 128\n","\n","# 11 is the last layer\n","layer = model.encoder.transformer.layers[11].final_layer_norm\n","hook_handle = layer.register_forward_hook(hook)\n","special_tokens= torch.load('./special_token.pt')\n","for path, transcript, target in zip(audio_paths_train,transcripts_train, targets_train):\n","    waveform, _ = torchaudio.load(path)\n","    emissions, _ = model(waveform.to(device))\n","    print(emissions.shape)\n","    out = layer_output\n","    out = out[0].detach().cpu()\n","    chars_to_ignore_regex = '[\\,\\?\\.\\-\\;\\:\\’]'\n","    transcript = re.sub(chars_to_ignore_regex, '', transcript)\n","    transcript = re.sub(\"\\s\", \"|\",transcript).upper()\n","    dictionary = {c: i for i, c in enumerate(labels)}\n","    tokens = [dictionary[c] for c in transcript]\n","    \n","    trellis = get_trellis(out, tokens)\n","    transition_path = backtrack(trellis, out, tokens)\n","    segments = merge_repeats(transition_path, transcript)\n","    word_segments = merge_words(segments)\n","    word_embeds = []\n","    for word in word_segments:\n","        word_embeds.append(torch.mean(out[word.start:word.end],0,True))\n","\n","    word_embeds = torch.stack(word_embeds)[0]\n","\n","    pad = torch.stack([special_tokens['[PAD]'] for _ in range(maxlen - len(word_embeds))])\n","    tokenized_sentence = torch.cat((word_embeds,pad))\n","\n","    label_ids = torch.tensor(target)\n","    pad = torch.tensor([label2id['O'] for _ in range(maxlen - len(target))])\n","    label_ids = torch.cat((label_ids,pad))\n","\n","    break\n","\n","\n","hook_handle.remove()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"asr-ner","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
